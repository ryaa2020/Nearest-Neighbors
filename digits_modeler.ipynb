{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# hw4pr2digits_modeler:  handwritten-digit modeling with k-nearest neighbors...\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# SUGGESTION:  \n",
    "# \n",
    "# +++ copy-paste-and-alter from the iris- + births-cleaning notebooks into here +++\n",
    "#\n",
    "# when the data is ready to view, you might want to grab\n",
    "# the digits-visualization code    (it was in hw3pr2.ipynb)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries!\n",
    "import numpy as np      # numpy is Python's \"array\" library\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digits_cleaned.csv : file read into a pandas dataframe.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix0</th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "      <th>...</th>\n",
       "      <th>pix55</th>\n",
       "      <th>pix56</th>\n",
       "      <th>pix57</th>\n",
       "      <th>pix58</th>\n",
       "      <th>pix59</th>\n",
       "      <th>pix60</th>\n",
       "      <th>pix61</th>\n",
       "      <th>pix62</th>\n",
       "      <th>pix63</th>\n",
       "      <th>actual_digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1768 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pix0  pix1  pix2  pix3  pix4  pix5  pix6  pix7  pix8  pix9  ...  pix55  \\\n",
       "0        0     0     9    14     8     1     0     0     0     0  ...      0   \n",
       "1        0     0    11    12     0     0     0     0     0     2  ...      0   \n",
       "2        0     0     1     9    15    11     0     0     0     0  ...      0   \n",
       "3        0     0     0     0    14    13     1     0     0     0  ...      0   \n",
       "4        0     0     5    12     1     0     0     0     0     0  ...      2   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "1763     0     0     4    10    13     6     0     0     0     1  ...      0   \n",
       "1764     0     0     6    16    13    11     1     0     0     0  ...      0   \n",
       "1765     0     0     1    11    15     1     0     0     0     0  ...      0   \n",
       "1766     0     0     2    10     7     0     0     0     0     0  ...      0   \n",
       "1767     0     0    10    14     8     1     0     0     0     2  ...      0   \n",
       "\n",
       "      pix56  pix57  pix58  pix59  pix60  pix61  pix62  pix63  actual_digit  \n",
       "0         0      0     11     16     15     11      1      0             8  \n",
       "1         0      0      9     12     13      3      0      0             9  \n",
       "2         0      0      1     10     13      3      0      0             0  \n",
       "3         0      0      0      1     13     16      1      0             1  \n",
       "4         0      0      3     11      8     13     12      4             2  \n",
       "...     ...    ...    ...    ...    ...    ...    ...    ...           ...  \n",
       "1763      0      0      2     14     15      9      0      0             9  \n",
       "1764      0      0      6     16     14      6      0      0             0  \n",
       "1765      0      0      2      9     13      6      0      0             8  \n",
       "1766      0      0      5     12     16     12      0      0             9  \n",
       "1767      0      1      8     12     14     12      1      0             8  \n",
       "\n",
       "[1768 rows x 65 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_filename = \"digits_cleaned.csv\"\n",
    "df_tidy = pd.read_csv(cleaned_filename)   # encoding=\"utf-8\" et al.\n",
    "print(f\"{cleaned_filename} : file read into a pandas dataframe.\")\n",
    "df_tidy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1768 entries, 0 to 1767\n",
      "Data columns (total 65 columns):\n",
      " #   Column        Non-Null Count  Dtype\n",
      "---  ------        --------------  -----\n",
      " 0   pix0          1768 non-null   int64\n",
      " 1   pix1          1768 non-null   int64\n",
      " 2   pix2          1768 non-null   int64\n",
      " 3   pix3          1768 non-null   int64\n",
      " 4   pix4          1768 non-null   int64\n",
      " 5   pix5          1768 non-null   int64\n",
      " 6   pix6          1768 non-null   int64\n",
      " 7   pix7          1768 non-null   int64\n",
      " 8   pix8          1768 non-null   int64\n",
      " 9   pix9          1768 non-null   int64\n",
      " 10  pix10         1768 non-null   int64\n",
      " 11  pix11         1768 non-null   int64\n",
      " 12  pix12         1768 non-null   int64\n",
      " 13  pix13         1768 non-null   int64\n",
      " 14  pix14         1768 non-null   int64\n",
      " 15  pix15         1768 non-null   int64\n",
      " 16  pix16         1768 non-null   int64\n",
      " 17  pix17         1768 non-null   int64\n",
      " 18  pix18         1768 non-null   int64\n",
      " 19  pix19         1768 non-null   int64\n",
      " 20  pix20         1768 non-null   int64\n",
      " 21  pix21         1768 non-null   int64\n",
      " 22  pix22         1768 non-null   int64\n",
      " 23  pix23         1768 non-null   int64\n",
      " 24  pix24         1768 non-null   int64\n",
      " 25  pix25         1768 non-null   int64\n",
      " 26  pix26         1768 non-null   int64\n",
      " 27  pix27         1768 non-null   int64\n",
      " 28  pix28         1768 non-null   int64\n",
      " 29  pix29         1768 non-null   int64\n",
      " 30  pix30         1768 non-null   int64\n",
      " 31  pix31         1768 non-null   int64\n",
      " 32  pix32         1768 non-null   int64\n",
      " 33  pix33         1768 non-null   int64\n",
      " 34  pix34         1768 non-null   int64\n",
      " 35  pix35         1768 non-null   int64\n",
      " 36  pix36         1768 non-null   int64\n",
      " 37  pix37         1768 non-null   int64\n",
      " 38  pix38         1768 non-null   int64\n",
      " 39  pix39         1768 non-null   int64\n",
      " 40  pix40         1768 non-null   int64\n",
      " 41  pix41         1768 non-null   int64\n",
      " 42  pix42         1768 non-null   int64\n",
      " 43  pix43         1768 non-null   int64\n",
      " 44  pix44         1768 non-null   int64\n",
      " 45  pix45         1768 non-null   int64\n",
      " 46  pix46         1768 non-null   int64\n",
      " 47  pix47         1768 non-null   int64\n",
      " 48  pix48         1768 non-null   int64\n",
      " 49  pix49         1768 non-null   int64\n",
      " 50  pix50         1768 non-null   int64\n",
      " 51  pix51         1768 non-null   int64\n",
      " 52  pix52         1768 non-null   int64\n",
      " 53  pix53         1768 non-null   int64\n",
      " 54  pix54         1768 non-null   int64\n",
      " 55  pix55         1768 non-null   int64\n",
      " 56  pix56         1768 non-null   int64\n",
      " 57  pix57         1768 non-null   int64\n",
      " 58  pix58         1768 non-null   int64\n",
      " 59  pix59         1768 non-null   int64\n",
      " 60  pix60         1768 non-null   int64\n",
      " 61  pix61         1768 non-null   int64\n",
      " 62  pix62         1768 non-null   int64\n",
      " 63  pix63         1768 non-null   int64\n",
      " 64  actual_digit  1768 non-null   int64\n",
      "dtypes: int64(65)\n",
      "memory usage: 911.6 KB\n"
     ]
    }
   ],
   "source": [
    "df_tidy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS is Index(['pix0', 'pix1', 'pix2', 'pix3', 'pix4', 'pix5', 'pix6', 'pix7', 'pix8',\n",
      "       'pix9', 'pix10', 'pix11', 'pix12', 'pix13', 'pix14', 'pix15', 'pix16',\n",
      "       'pix17', 'pix18', 'pix19', 'pix20', 'pix21', 'pix22', 'pix23', 'pix24',\n",
      "       'pix25', 'pix26', 'pix27', 'pix28', 'pix29', 'pix30', 'pix31', 'pix32',\n",
      "       'pix33', 'pix34', 'pix35', 'pix36', 'pix37', 'pix38', 'pix39', 'pix40',\n",
      "       'pix41', 'pix42', 'pix43', 'pix44', 'pix45', 'pix46', 'pix47', 'pix48',\n",
      "       'pix49', 'pix50', 'pix51', 'pix52', 'pix53', 'pix54', 'pix55', 'pix56',\n",
      "       'pix57', 'pix58', 'pix59', 'pix60', 'pix61', 'pix62', 'pix63',\n",
      "       'actual_digit'],\n",
      "      dtype='object')\n",
      "\n",
      "COLUMNS[0] is pix0\n",
      "\n",
      "COL_INDEX is {'pix0': 0, 'pix1': 1, 'pix2': 2, 'pix3': 3, 'pix4': 4, 'pix5': 5, 'pix6': 6, 'pix7': 7, 'pix8': 8, 'pix9': 9, 'pix10': 10, 'pix11': 11, 'pix12': 12, 'pix13': 13, 'pix14': 14, 'pix15': 15, 'pix16': 16, 'pix17': 17, 'pix18': 18, 'pix19': 19, 'pix20': 20, 'pix21': 21, 'pix22': 22, 'pix23': 23, 'pix24': 24, 'pix25': 25, 'pix26': 26, 'pix27': 27, 'pix28': 28, 'pix29': 29, 'pix30': 30, 'pix31': 31, 'pix32': 32, 'pix33': 33, 'pix34': 34, 'pix35': 35, 'pix36': 36, 'pix37': 37, 'pix38': 38, 'pix39': 39, 'pix40': 40, 'pix41': 41, 'pix42': 42, 'pix43': 43, 'pix44': 44, 'pix45': 45, 'pix46': 46, 'pix47': 47, 'pix48': 48, 'pix49': 49, 'pix50': 50, 'pix51': 51, 'pix52': 52, 'pix53': 53, 'pix54': 54, 'pix55': 55, 'pix56': 56, 'pix57': 57, 'pix58': 58, 'pix59': 59, 'pix60': 60, 'pix61': 61, 'pix62': 62, 'pix63': 63, 'actual_digit': 64}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# once we have all the columns we want, let's create an index of their names...\n",
    "\n",
    "#\n",
    "# Let's make sure we have all of our helpful variables in one place \n",
    "#       To be adapted if we drop/add more columns...\n",
    "#\n",
    "\n",
    "#\n",
    "# let's keep our column names in variables, for reference\n",
    "#\n",
    "df_model1 = df_tidy\n",
    "COLUMNS = df_model1.columns            # \"list\" of columns\n",
    "print(f\"COLUMNS is {COLUMNS}\\n\")  \n",
    "  # It's a \"pandas\" list, called an Index\n",
    "  # use it just as a Python list of strings:\n",
    "print(f\"COLUMNS[0] is {COLUMNS[0]}\\n\")\n",
    "\n",
    "# let's create a dictionary to look up any column index by name\n",
    "COL_INDEX = {}\n",
    "for i, name in enumerate(COLUMNS):\n",
    "    COL_INDEX[name] = i  # using the name (as key), look up the value (i)\n",
    "print(f\"COL_INDEX is {COL_INDEX}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  9 ...  1  0  8]\n",
      " [ 0  0 11 ...  0  0  9]\n",
      " [ 0  0  1 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  1 ...  0  0  8]\n",
      " [ 0  0  2 ...  0  0  9]\n",
      " [ 0  0 10 ...  1  0  8]]\n"
     ]
    }
   ],
   "source": [
    "A = df_model1.to_numpy()    # .values gets the numpy array\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  9. ...  1.  0.  8.]\n",
      " [ 0.  0. 11. ...  0.  0.  9.]\n",
      " [ 0.  0.  1. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  0.  0.  8.]\n",
      " [ 0.  0.  2. ...  0.  0.  9.]\n",
      " [ 0.  0. 10. ...  1.  0.  8.]]\n"
     ]
    }
   ],
   "source": [
    "A = A.astype('float64')  # so many types:  www.tutorialspoint.com/numpy/numpy_data_types.htm\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The dataset has 1768 rows and 65 cols\n"
     ]
    }
   ],
   "source": [
    "NUM_ROWS, NUM_COLS = A.shape\n",
    "print(f\"\\nThe dataset has {NUM_ROWS} rows and {NUM_COLS} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of data definitions +++\n",
      "\n",
      "y_all (just the labels/species)   are \n",
      " [8. 9. 0. ... 8. 9. 8.]\n",
      "X_all (just the features) are \n",
      " [[ 0.  0.  9. ... 11.  1.  0.]\n",
      " [ 0.  0. 11. ...  3.  0.  0.]\n",
      " [ 0.  0.  1. ...  3.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"+++ Start of data definitions +++\\n\")\n",
    "\n",
    "#\n",
    "# we could do this at the data-frame level, too!\n",
    "#\n",
    "\n",
    "X_all = A[:,0:64]  # X (features) ... is all rows, columns 0, 1, 2, 3\n",
    "y_all = A[:,64]    # y (labels) ... is all rows, column 4 only\n",
    "\n",
    "print(f\"y_all (just the labels/species)   are \\n {y_all}\")\n",
    "print(f\"X_all (just the features) are \\n {X_all}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scrambled labels/species are \n",
      " [6. 4. 9. ... 8. 3. 7.]\n",
      "The corresponding data rows are \n",
      " [[ 0.  0.  0. ... 14.  9.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  1. 14. ...  4.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  6. ...  1.  0.  0.]\n",
      " [ 0.  2. 13. ...  9.  0.  0.]\n",
      " [ 0.  0.  4. ...  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# we scramble the data, to remove (potential) dependence on its ordering: \n",
    "# \n",
    "indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "# we scramble both X and y, necessarily with the same permutation\n",
    "X_labeled = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "y_labeled = y_all[indices]              # again...\n",
    "print(f\"The scrambled labels/species are \\n {y_labeled}\")\n",
    "print(f\"The corresponding data rows are \\n {X_labeled}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with 1414 rows;  testing with 354 rows\n",
      "\n",
      "Held-out data... (testing data: 354)\n",
      "y_test: [5. 0. 2. 1. 9. 6. 4. 5. 7. 2. 6. 2. 7. 5. 9. 9. 4. 1. 2. 3. 1. 8. 5. 4.\n",
      " 2. 2. 7. 6. 6. 5. 8. 9. 8. 3. 4. 3. 0. 8. 7. 6. 4. 4. 1. 3. 1. 9. 1. 9.\n",
      " 3. 8. 5. 1. 0. 6. 4. 3. 2. 0. 8. 3. 5. 5. 8. 6. 7. 4. 9. 0. 9. 4. 0. 7.\n",
      " 0. 9. 7. 8. 5. 4. 0. 5. 3. 4. 4. 3. 6. 6. 2. 3. 7. 0. 2. 9. 2. 4. 3. 7.\n",
      " 8. 6. 7. 8. 5. 8. 2. 0. 5. 0. 6. 2. 4. 7. 3. 6. 1. 8. 7. 4. 3. 6. 3. 2.\n",
      " 8. 7. 2. 1. 1. 9. 4. 3. 2. 6. 3. 6. 3. 9. 1. 6. 2. 5. 7. 5. 4. 1. 1. 9.\n",
      " 2. 1. 9. 1. 9. 6. 3. 4. 4. 1. 0. 0. 2. 5. 3. 7. 5. 2. 8. 4. 0. 6. 6. 7.\n",
      " 8. 5. 9. 1. 2. 6. 8. 3. 3. 4. 2. 2. 5. 9. 3. 1. 5. 2. 5. 1. 4. 3. 2. 0.\n",
      " 0. 6. 4. 9. 3. 9. 4. 5. 4. 3. 9. 9. 3. 0. 8. 0. 4. 9. 1. 2. 3. 8. 9. 6.\n",
      " 2. 9. 2. 2. 9. 0. 1. 2. 3. 5. 4. 7. 0. 6. 5. 4. 7. 8. 3. 5. 5. 3. 0. 7.\n",
      " 0. 3. 1. 4. 4. 6. 0. 9. 6. 7. 6. 1. 7. 3. 1. 9. 9. 7. 9. 0. 8. 3. 3. 2.\n",
      " 6. 4. 3. 6. 4. 8. 6. 9. 0. 6. 1. 5. 2. 8. 6. 6. 4. 9. 0. 3. 7. 5. 3. 6.\n",
      " 1. 4. 9. 8. 2. 4. 1. 9. 3. 0. 7. 3. 5. 7. 9. 9. 6. 7. 9. 3. 8. 2. 2. 3.\n",
      " 3. 2. 1. 9. 5. 4. 7. 8. 9. 5. 1. 1. 9. 2. 4. 8. 9. 7. 6. 6. 7. 8. 8. 6.\n",
      " 7. 7. 5. 1. 7. 5. 3. 7. 1. 8. 4. 6. 0. 4. 1. 4. 8. 2.]\n",
      "X_test (a few rows): [[ 0.  3. 10. 11. 12. 12.  6.  0.  0.  8. 14. 11.  8.  8.  4.  0.  0.  8.\n",
      "  10.  7.  3.  0.  0.  0.  0.  8. 16. 14. 15.  4.  0.  0.  0.  2.  2.  0.\n",
      "   6.  9.  0.  0.  0.  0.  0.  0.  4. 12.  0.  0.  0.  1.  8.  4. 10. 10.\n",
      "   0.  0.  0.  2. 15. 16. 13.  2.  0.  0.]\n",
      " [ 0.  0.  2. 13. 10.  3.  0.  0.  0.  0. 10. 15. 12. 13.  1.  0.  0.  0.\n",
      "  16.  4.  0.  6.  4.  0.  0.  2. 16.  3.  0.  1.  7.  0.  0.  5. 13.  5.\n",
      "   0.  2.  8.  0.  0.  4. 12.  0.  0.  3.  8.  0.  0.  0. 13.  5.  6. 13.\n",
      "   5.  0.  0.  0.  5. 14. 13.  8.  1.  0.]\n",
      " [ 0.  0.  7. 13.  2.  0.  0.  0.  0. 11. 15. 12. 13.  0.  0.  0.  0. 12.\n",
      "   7.  0. 16.  4.  0.  0.  0.  4.  4.  0. 14.  8.  0.  0.  0.  0.  0.  0.\n",
      "  14.  7.  0.  0.  0.  0.  0.  4. 16.  3.  0.  0.  0.  0. 12. 16. 16. 12.\n",
      "   9.  0.  0.  0.  9. 12.  8. 10. 14.  0.]\n",
      " [ 0.  0. 10. 13.  1.  0.  0.  0.  0.  0.  7. 16.  5.  0.  0.  0.  0.  0.\n",
      "   6. 16.  6.  0.  0.  0.  0.  0.  6. 16. 13.  0.  0.  0.  0.  0.  0.  6.\n",
      "  16.  2.  0.  0.  0.  0.  0.  3. 16.  8.  0.  0.  0.  0.  7. 11. 16. 14.\n",
      "   9.  4.  0.  0.  6. 15. 13. 14. 16. 15.]\n",
      " [ 0.  0.  2. 10. 16. 11.  1.  0.  0.  0. 13. 13. 10. 16.  8.  0.  0.  4.\n",
      "  14.  1.  8. 14.  1.  0.  0.  4. 15. 12. 15.  8.  0.  0.  0.  0.  6.  7.\n",
      "  14.  5.  0.  0.  0.  1.  2.  0. 12.  5.  0.  0.  0.  8. 15.  6. 13.  4.\n",
      "   0.  0.  0.  0.  5. 11. 16.  3.  0.  0.]]\n",
      "\n",
      "Data used for modeling... (training data: 1414)\n",
      "y_train: [8. 3. 7. ... 2. 9. 9.]\n",
      "X_train (a few rows): [[ 0.  0.  2. 12. 16. 10.  0.  0.  0.  3. 15. 10.  7. 16.  4.  0.  0.  9.\n",
      "   8.  0. 11. 10.  0.  0.  0.  3. 15. 11. 14.  1.  0.  0.  0.  0. 10. 16.\n",
      "   9.  0.  0.  0.  0.  0. 14.  7. 13.  4.  0.  0.  0.  0.  9.  7.  6. 10.\n",
      "   0.  0.  0.  0.  1. 12. 16.  5.  0.  0.]\n",
      " [ 0.  1. 11. 16. 13.  4.  0.  0.  0.  1. 15.  7. 14. 14.  1.  0.  0.  0.\n",
      "   0.  0.  6. 15.  1.  0.  0.  0.  1. 10. 15.  6.  0.  0.  0.  0.  5. 15.\n",
      "  14.  7.  0.  0.  0.  0.  1.  0.  5. 16.  3.  0.  0.  5. 11.  1.  1. 16.\n",
      "   4.  0.  0.  0. 10. 15. 16. 10.  1.  0.]\n",
      " [ 0.  0. 12. 16. 16.  5.  0.  0.  0.  3. 13.  8. 14. 15.  1.  0.  0.  0.\n",
      "   0.  0. 13. 16.  0.  0.  0.  6. 16. 16. 16. 16. 13.  0.  0.  6.  9. 11.\n",
      "  16.  9.  5.  0.  0.  0.  0. 14. 11.  0.  0.  0.  0.  0.  7. 16.  2.  0.\n",
      "   0.  0.  0.  0. 13. 10.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  3. 16.  2.  0.  0.  0.  0.  0. 12. 12.  0.  0.  0.  0.  0.\n",
      "   5. 16.  2.  5. 12.  0.  0.  3. 15.  8.  0. 11. 13.  0.  0.  9. 16.  4.\n",
      "   7. 16.  8.  0.  0.  9. 16. 16. 16. 16.  2.  0.  0.  0.  0.  0. 13. 12.\n",
      "   0.  0.  0.  0.  0.  1. 16.  9.  0.  0.]\n",
      " [ 0.  0.  0.  7. 16. 16.  7.  0.  0.  0.  0. 14. 16. 16.  4.  0.  0.  2.\n",
      "  13. 16. 16. 12.  0.  0.  0.  7. 16. 16. 16. 12.  0.  0.  0.  0.  0. 10.\n",
      "  16.  8.  0.  0.  0.  0.  0. 11. 16. 13.  0.  0.  0.  0.  0. 10. 16. 16.\n",
      "   2.  0.  0.  0.  0.  9. 16. 12.  2.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# We next separate into test data and training data ... \n",
    "#    + We will train on the training data...\n",
    "#    + We will _not_ look at the testing data to build the model\n",
    "#\n",
    "# Then, afterward, we will test on the testing data -- and see how well we do!\n",
    "#\n",
    "\n",
    "#\n",
    "# a common convention:  train on 80%, test on 20%    Let's define the TEST_PERCENT\n",
    "#\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"training with {len(y_train)} rows;  testing with {len(y_test)} rows\\n\" )\n",
    "\n",
    "print(f\"Held-out data... (testing data: {len(y_test)})\")\n",
    "print(f\"y_test: {y_test}\")\n",
    "print(f\"X_test (a few rows): {X_test[0:5,:]}\")  # 5 rows\n",
    "print()\n",
    "print(f\"Data used for modeling... (training data: {len(y_train)})\")\n",
    "print(f\"y_train: {y_train}\")\n",
    "print(f\"X_train (a few rows): {X_train[0:5,:]}\")  # 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created and trained a knn classifier with k = 5\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# +++ This is the \"Model-building and Model-training Cell\"\n",
    "#       \n",
    "# Create a kNN model and train it! \n",
    "#\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "k = 5  # we don't know what k to use, so we guess!  (this will _not_ be a good value)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=k)       # here, k is the \"k\" in kNN\n",
    "\n",
    "# we train the model (it's one line!)\n",
    "knn_model.fit(X_train, y_train)                              # yay!  trained!\n",
    "print(\"Created and trained a knn classifier with k =\", k)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:  1  cv accuracy:  0.9866\n",
      "k:  2  cv accuracy:  0.9830\n",
      "k:  3  cv accuracy:  0.9837\n",
      "k:  4  cv accuracy:  0.9816\n",
      "k:  5  cv accuracy:  0.9844\n",
      "k:  6  cv accuracy:  0.9802\n",
      "k:  7  cv accuracy:  0.9809\n",
      "k:  8  cv accuracy:  0.9809\n",
      "k:  9  cv accuracy:  0.9823\n",
      "k: 10  cv accuracy:  0.9816\n",
      "k: 11  cv accuracy:  0.9788\n",
      "k: 12  cv accuracy:  0.9760\n",
      "k: 13  cv accuracy:  0.9724\n",
      "k: 14  cv accuracy:  0.9696\n",
      "k: 15  cv accuracy:  0.9689\n",
      "k: 16  cv accuracy:  0.9675\n",
      "k: 17  cv accuracy:  0.9675\n",
      "k: 18  cv accuracy:  0.9668\n",
      "k: 19  cv accuracy:  0.9653\n",
      "k: 20  cv accuracy:  0.9639\n",
      "k: 21  cv accuracy:  0.9632\n",
      "k: 22  cv accuracy:  0.9625\n",
      "k: 23  cv accuracy:  0.9611\n",
      "k: 24  cv accuracy:  0.9611\n",
      "k: 25  cv accuracy:  0.9618\n",
      "k: 26  cv accuracy:  0.9604\n",
      "k: 27  cv accuracy:  0.9590\n",
      "k: 28  cv accuracy:  0.9561\n",
      "k: 29  cv accuracy:  0.9576\n",
      "k: 30  cv accuracy:  0.9561\n",
      "k: 31  cv accuracy:  0.9533\n",
      "k: 32  cv accuracy:  0.9533\n",
      "k: 33  cv accuracy:  0.9505\n",
      "k: 34  cv accuracy:  0.9498\n",
      "k: 35  cv accuracy:  0.9484\n",
      "k: 36  cv accuracy:  0.9491\n",
      "k: 37  cv accuracy:  0.9470\n",
      "k: 38  cv accuracy:  0.9441\n",
      "k: 39  cv accuracy:  0.9441\n",
      "k: 40  cv accuracy:  0.9420\n",
      "k: 41  cv accuracy:  0.9420\n",
      "k: 42  cv accuracy:  0.9427\n",
      "k: 43  cv accuracy:  0.9420\n",
      "k: 44  cv accuracy:  0.9427\n",
      "k: 45  cv accuracy:  0.9413\n",
      "k: 46  cv accuracy:  0.9406\n",
      "k: 47  cv accuracy:  0.9399\n",
      "k: 48  cv accuracy:  0.9399\n",
      "k: 49  cv accuracy:  0.9378\n",
      "k: 50  cv accuracy:  0.9385\n",
      "k: 51  cv accuracy:  0.9378\n",
      "k: 52  cv accuracy:  0.9385\n",
      "k: 53  cv accuracy:  0.9363\n",
      "k: 54  cv accuracy:  0.9378\n",
      "k: 55  cv accuracy:  0.9363\n",
      "k: 56  cv accuracy:  0.9349\n",
      "k: 57  cv accuracy:  0.9342\n",
      "k: 58  cv accuracy:  0.9328\n",
      "k: 59  cv accuracy:  0.9328\n",
      "k: 60  cv accuracy:  0.9321\n",
      "k: 61  cv accuracy:  0.9328\n",
      "k: 62  cv accuracy:  0.9300\n",
      "k: 63  cv accuracy:  0.9314\n",
      "k: 64  cv accuracy:  0.9293\n",
      "k: 65  cv accuracy:  0.9307\n",
      "k: 66  cv accuracy:  0.9286\n",
      "k: 67  cv accuracy:  0.9264\n",
      "k: 68  cv accuracy:  0.9257\n",
      "k: 69  cv accuracy:  0.9257\n",
      "k: 70  cv accuracy:  0.9293\n",
      "k: 71  cv accuracy:  0.9272\n",
      "k: 72  cv accuracy:  0.9264\n",
      "k: 73  cv accuracy:  0.9250\n",
      "k: 74  cv accuracy:  0.9215\n",
      "k: 75  cv accuracy:  0.9194\n",
      "k: 76  cv accuracy:  0.9208\n",
      "k: 77  cv accuracy:  0.9187\n",
      "k: 78  cv accuracy:  0.9187\n",
      "k: 79  cv accuracy:  0.9180\n",
      "k: 80  cv accuracy:  0.9165\n",
      "k: 81  cv accuracy:  0.9144\n",
      "k: 82  cv accuracy:  0.9158\n",
      "k: 83  cv accuracy:  0.9172\n",
      "k: 84  cv accuracy:  0.9165\n",
      "best_k = 1   yields the highest average cv accuracy.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# to do this, we use \"cross validation\"\n",
    "#\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#\n",
    "# cross-validation splits the training set into two pieces:\n",
    "#   + model-building and model-validation. We'll use \"build\" and \"validate\"\n",
    "#\n",
    "best_k = 84 # Not correct!\n",
    "best_accuracy = 0.0  # also not correct...\n",
    "\n",
    "# Note that we are cross-validating using only our TEST data!\n",
    "for k in range(1,85):\n",
    "    knn_cv_model = KNeighborsClassifier(n_neighbors=k)   # build knn_model for every k!\n",
    "    cv_scores = cross_val_score( knn_cv_model, X_train, y_train, cv=5 )  # cv=5 means 80/20\n",
    "    # print(cv_scores)  # just to see the five scores... \n",
    "    average_cv_accuracy = cv_scores.mean()  # mean() is numpy's built-in average function \n",
    "    print(f\"k: {k:2d}  cv accuracy: {average_cv_accuracy:7.4f}\")\n",
    "\n",
    "    \n",
    "# assign best value of k to best_k\n",
    "    if average_cv_accuracy > best_accuracy:\n",
    "        best_k = k    # at the moment this is incorrect   TO DO for hw4pr1: fix this...\n",
    "        best_accuracy = average_cv_accuracy\n",
    "# you'll need to use the loop above to find and remember the real best_k\n",
    "\n",
    "print(f\"best_k = {best_k}   yields the highest average cv accuracy.\")  # print the best one\n",
    "\n",
    "#\n",
    "# Your task:  \n",
    "#    is to use this loop to remember the best value of k in the variable best_k\n",
    "#    (feel free to use best_k and best_accuracy, above)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created + trained a 'final' knn classifier, with a (best) k of 1\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Ok!  We have tuned knn to use the \"best\" value of k...\n",
    "#\n",
    "# And, we should now use ALL available data to train our final predictive model:\n",
    "#\n",
    "\n",
    "knn_model_final = KNeighborsClassifier(n_neighbors=best_k)   # here, we use the best_k\n",
    "knn_model_final.fit(X_all, y_all)                              # here we use ALL the data!\n",
    "print(f\"Created + trained a 'final' knn classifier, with a (best) k of {best_k}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [5. 0. 2. 1. 8. 6. 4. 5. 7. 2. 6. 2. 7. 5. 9. 9. 4. 1. 2. 3. 1. 8. 5. 4.\n",
      " 2. 2. 7. 6. 6. 5. 8. 9. 8. 3. 4. 3. 0. 8. 7. 6. 4. 4. 1. 3. 1. 9. 1. 5.\n",
      " 3. 8. 5. 1. 0. 6. 4. 3. 2. 0. 8. 3. 5. 5. 8. 6. 7. 4. 9. 0. 9. 4. 0. 7.\n",
      " 0. 9. 7. 8. 5. 4. 0. 5. 3. 4. 4. 3. 6. 6. 2. 3. 7. 0. 2. 9. 2. 4. 3. 7.\n",
      " 8. 6. 7. 8. 5. 8. 2. 0. 5. 0. 6. 2. 4. 7. 3. 6. 1. 8. 7. 4. 3. 6. 3. 2.\n",
      " 8. 7. 2. 1. 1. 9. 4. 3. 2. 6. 3. 6. 3. 9. 1. 6. 2. 5. 7. 5. 4. 1. 1. 9.\n",
      " 2. 1. 9. 1. 9. 6. 3. 4. 4. 1. 0. 0. 2. 5. 3. 7. 5. 2. 8. 4. 0. 6. 6. 7.\n",
      " 8. 5. 9. 1. 2. 6. 8. 3. 3. 4. 2. 2. 5. 9. 3. 1. 5. 2. 5. 1. 4. 3. 2. 0.\n",
      " 0. 6. 4. 9. 3. 9. 4. 5. 4. 3. 9. 9. 3. 0. 8. 0. 4. 9. 1. 2. 3. 8. 9. 6.\n",
      " 2. 9. 2. 2. 9. 0. 1. 2. 3. 5. 4. 7. 0. 6. 5. 4. 7. 8. 3. 5. 5. 3. 0. 7.\n",
      " 0. 3. 1. 4. 4. 6. 0. 9. 6. 7. 6. 1. 7. 3. 1. 9. 9. 7. 9. 0. 8. 3. 3. 2.\n",
      " 6. 4. 3. 6. 4. 8. 6. 9. 0. 1. 1. 5. 2. 8. 6. 6. 4. 9. 0. 3. 7. 5. 3. 6.\n",
      " 1. 4. 9. 8. 2. 4. 1. 9. 3. 0. 7. 3. 5. 7. 9. 1. 6. 7. 9. 3. 8. 2. 2. 3.\n",
      " 3. 2. 1. 9. 5. 4. 7. 8. 9. 5. 1. 1. 9. 2. 4. 8. 9. 7. 6. 6. 7. 8. 8. 6.\n",
      " 7. 7. 5. 1. 7. 5. 3. 7. 1. 8. 4. 6. 0. 4. 1. 4. 8. 2.]\n",
      "Actual  labels  : [5. 0. 2. 1. 9. 6. 4. 5. 7. 2. 6. 2. 7. 5. 9. 9. 4. 1. 2. 3. 1. 8. 5. 4.\n",
      " 2. 2. 7. 6. 6. 5. 8. 9. 8. 3. 4. 3. 0. 8. 7. 6. 4. 4. 1. 3. 1. 9. 1. 9.\n",
      " 3. 8. 5. 1. 0. 6. 4. 3. 2. 0. 8. 3. 5. 5. 8. 6. 7. 4. 9. 0. 9. 4. 0. 7.\n",
      " 0. 9. 7. 8. 5. 4. 0. 5. 3. 4. 4. 3. 6. 6. 2. 3. 7. 0. 2. 9. 2. 4. 3. 7.\n",
      " 8. 6. 7. 8. 5. 8. 2. 0. 5. 0. 6. 2. 4. 7. 3. 6. 1. 8. 7. 4. 3. 6. 3. 2.\n",
      " 8. 7. 2. 1. 1. 9. 4. 3. 2. 6. 3. 6. 3. 9. 1. 6. 2. 5. 7. 5. 4. 1. 1. 9.\n",
      " 2. 1. 9. 1. 9. 6. 3. 4. 4. 1. 0. 0. 2. 5. 3. 7. 5. 2. 8. 4. 0. 6. 6. 7.\n",
      " 8. 5. 9. 1. 2. 6. 8. 3. 3. 4. 2. 2. 5. 9. 3. 1. 5. 2. 5. 1. 4. 3. 2. 0.\n",
      " 0. 6. 4. 9. 3. 9. 4. 5. 4. 3. 9. 9. 3. 0. 8. 0. 4. 9. 1. 2. 3. 8. 9. 6.\n",
      " 2. 9. 2. 2. 9. 0. 1. 2. 3. 5. 4. 7. 0. 6. 5. 4. 7. 8. 3. 5. 5. 3. 0. 7.\n",
      " 0. 3. 1. 4. 4. 6. 0. 9. 6. 7. 6. 1. 7. 3. 1. 9. 9. 7. 9. 0. 8. 3. 3. 2.\n",
      " 6. 4. 3. 6. 4. 8. 6. 9. 0. 6. 1. 5. 2. 8. 6. 6. 4. 9. 0. 3. 7. 5. 3. 6.\n",
      " 1. 4. 9. 8. 2. 4. 1. 9. 3. 0. 7. 3. 5. 7. 9. 9. 6. 7. 9. 3. 8. 2. 2. 3.\n",
      " 3. 2. 1. 9. 5. 4. 7. 8. 9. 5. 1. 1. 9. 2. 4. 8. 9. 7. 6. 6. 7. 8. 8. 6.\n",
      " 7. 7. 5. 1. 7. 5. 3. 7. 1. 8. 4. 6. 0. 4. 1. 4. 8. 2.]\n",
      "\n",
      "Results on test set:  350 correct out of 354 total.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# +++ This cell will \"Model-testing Cell\"\n",
    "#\n",
    "# Now, let's see how well our model does on our \"held-out data\" (the testing data)\n",
    "#\n",
    "\n",
    "# We run our test set:\n",
    "\n",
    "# the function knn_model.predict is the instantiation of our model\n",
    "# it's what runs the k-nearest-neighbors algorithm:\n",
    "predicted_labels = knn_model.predict(X_test)   \n",
    "actual_labels = y_test\n",
    "\n",
    "# Let's print them so we can compare...\n",
    "print(\"Predicted labels:\", predicted_labels)\n",
    "print(\"Actual  labels  :\", actual_labels)\n",
    "\n",
    "# And, some overall results\n",
    "num_correct = sum(predicted_labels == actual_labels)\n",
    "total = len(actual_labels)\n",
    "print(f\"\\nResults on test set:  {num_correct} correct out of {total} total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I predict 4 from the features [0, 0, 0, 8, 14, 0, 0, 0, 0, 0, 5, 16, 11, 0, 0, 0, 0, 1, 15, 14, 1, 6, 0, 0, 0, 7, 16, 5, 3, 16, 8, 0, 0, 8, 16, 8, 14, 16, 2, 0, 0, 0, 6, 14, 16, 11, 0, 0, 0, 0, 0, 6, 16, 4, 0, 0, 0, 0, 0, 10, 15, 0, 0, 0]\n",
      "I predict 2 from the features [0, 0, 0, 5, 14, 12, 2, 0, 0, 0, 7, 15, 8, 14, 4, 0, 0, 0, 6, 2, 3, 13, 1, 0, 0, 0, 0, 1, 13, 4, 0, 0, 0, 0, 1, 11, 9, 0, 0, 0, 0, 8, 16, 13, 0, 0, 0, 0, 0, 5, 14, 16, 11, 2, 0, 0, 0, 0, 0, 6, 12, 13, 3, 0]\n",
      "I predict 4 from the features [0, 0, 0, 3, 16, 3, 0, 0, 0, 0, 0, 12, 16, 2, 0, 0, 0, 0, 8, 16, 16, 4, 0, 0, 0, 7, 16, 15, 16, 12, 11, 0, 0, 8, 16, 16, 16, 13, 3, 0, 0, 0, 0, 7, 14, 1, 0, 0, 0, 0, 0, 6, 16, 0, 0, 0, 0, 0, 0, 4, 14, 0, 0, 0]\n",
      "I predict 2 from the features [0, 0, 0, 3, 15, 10, 1, 0, 0, 0, 0, 11, 10, 16, 4, 0, 0, 0, 0, 12, 1, 15, 6, 0, 0, 0, 0, 3, 4, 15, 4, 0, 0, 0, 0, 6, 15, 6, 0, 0, 0, 4, 15, 16, 9, 0, 0, 0, 0, 0, 13, 16, 15, 9, 3, 0, 0, 0, 0, 4, 9, 14, 7, 0]\n",
      "I predict 4 from the features [0, 0, 0, 3, 16, 3, 0, 0, 0, 0, 0, 10, 16, 11, 0, 0, 0, 0, 4, 16, 16, 8, 0, 0, 0, 2, 14, 12, 16, 5, 0, 0, 0, 10, 16, 14, 16, 16, 11, 0, 0, 5, 12, 13, 16, 8, 3, 0, 0, 0, 0, 2, 15, 3, 0, 0, 0, 0, 0, 4, 12, 0, 0, 0]\n",
      "I predict 2 from the features [0, 0, 7, 15, 15, 4, 0, 0, 0, 8, 16, 16, 16, 4, 0, 0, 0, 8, 15, 8, 16, 4, 0, 0, 0, 0, 0, 10, 15, 0, 0, 0, 0, 0, 1, 15, 9, 0, 0, 0, 0, 0, 6, 16, 2, 0, 0, 0, 0, 0, 8, 16, 8, 11, 9, 0, 0, 0, 9, 16, 16, 12, 3, 0]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# final predictive model (k-nearest-neighbor), with tuned k + ALL data incorporated\n",
    "#\n",
    "\n",
    "def predictive_model( Features, Model ):\n",
    "    \"\"\" input: a list of 64 ints, each representing the cof a pixel \n",
    "                \n",
    "        output: the predicted digit from the list of pixel values\n",
    "                 \n",
    "    \"\"\"\n",
    "    our_features = np.asarray([Features])                 # extra brackets needed\n",
    "\n",
    "    # The model's prediction!\n",
    "    predicted_digit = Model.predict(our_features)\n",
    "    \n",
    "    # a bit awkward\n",
    "    predicted_digit = int(round(predicted_digit[0]))  # unpack one element\n",
    "    return predicted_digit\n",
    "\n",
    "\n",
    "#\n",
    "# Try it!\n",
    "# \n",
    "# Features = eval(input(\"Enter new Features: \"))\n",
    "#\n",
    "# Features = [6.7,3.3,5.7,0.1]  # [5.8,2.7,4.1,1.0] [4.6,3.6,3.0,2.2] [6.7,3.3,5.7,2.1]\n",
    "\n",
    "LoF = [[0,0,0,8,14,0,0,0,0,0,5,16,11,0,0,0,0,1,15,14,1,6,0,0,0,7,16,5,3,16,8,0,0,8,16,8,14,16,2,0,0,0,6,14,16,11,0,0,0,0,0,6,16,4,0,0,0,0,0,10,15,0,0,0],\n",
    "[0,0,0,5,14,12,2,0,0,0,7,15,8,14,4,0,0,0,6,2,3,13,1,0,0,0,0,1,13,4,0,0,0,0,1,11,9,0,0,0,0,8,16,13,0,0,0,0,0,5,14,16,11,2,0,0,0,0,0,6,12,13,3,0],\n",
    "[0,0,0,3,16,3,0,0,0,0,0,12,16,2,0,0,0,0,8,16,16,4,0,0,0,7,16,15,16,12,11,0,0,8,16,16,16,13,3,0,0,0,0,7,14,1,0,0,0,0,0,6,16,0,0,0,0,0,0,4,14,0,0,0],\n",
    "[0,0,0,3,15,10,1,0,0,0,0,11,10,16,4,0,0,0,0,12,1,15,6,0,0,0,0,3,4,15,4,0,0,0,0,6,15,6,0,0,0,4,15,16,9,0,0,0,0,0,13,16,15,9,3,0,0,0,0,4,9,14,7,0],\n",
    "[0,0,0,3,16,3,0,0,0,0,0,10,16,11,0,0,0,0,4,16,16,8,0,0,0,2,14,12,16,5,0,0,0,10,16,14,16,16,11,0,0,5,12,13,16,8,3,0,0,0,0,2,15,3,0,0,0,0,0,4,12,0,0,0],\n",
    "[0,0,7,15,15,4,0,0,0,8,16,16,16,4,0,0,0,8,15,8,16,4,0,0,0,0,0,10,15,0,0,0,0,0,1,15,9,0,0,0,0,0,6,16,2,0,0,0,0,0,8,16,8,11,9,0,0,0,9,16,16,12,3,0]]\n",
    "  \n",
    "# run on each one:\n",
    "for Features in LoF:\n",
    "    predicted_digit = predictive_model( Features, knn_model_final )  # pass in the model, too!\n",
    "    print(f\"I predict {predicted_digit} from the features {Features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Digit-recognition experiments:\n",
    "+ abstract the above process into a function, so that you can run the digit-modeling on <i>different numbers of pixel-rows</i> !\n",
    "  + pixels 0..8 are the first row  (excluding 8)\n",
    "  + pixels 0..16  are the first two rows... (excluding 16)\n",
    "  + pixels 0..24  are the first three rows... (excluding 24)\n",
    "  + and so on...\n",
    "+ Run for pixels in the first row, first two rows, first three rows, etc.\n",
    "+ Share your results (in an ASCII table - or, optionally, a lineplot! - of how the number of pixels available affects the recognition-accuracy)\n",
    "+ This provides insight about the <u>redundancy</u> of our glyphs for digits (or letters) Some scripts/alphabets offer more redundancy than others (depending on <i>how</i> information is removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ Start of data definitions +++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"+++ Start of data definitions +++\\n\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#\n",
    "# we could do this at the data-frame level, too!\n",
    "#\n",
    "def mask_digit(pixel):\n",
    "    X_all = A[:,0:pixel]  # X (features) ... is all rows, columns 0, 1, 2, 3\n",
    "    y_all = A[:,64]    # y (labels) ... is all rows, column 4 only\n",
    "\n",
    "    # print(f\"y_all (just the labels/species)   are \\n {y_all}\")\n",
    "    # print(f\"X_all (just the features) are \\n {X_all}\")\n",
    "    \n",
    "    # we scramble the data, to remove (potential) dependence on its ordering: \n",
    "    # \n",
    "    indices = np.random.permutation(len(y_all))  # indices is a permutation-list\n",
    "\n",
    "    # we scramble both X and y, necessarily with the same permutation\n",
    "    X_labeled = X_all[indices]              # we apply the _same_ permutation to each!\n",
    "    y_labeled = y_all[indices]              # again...\n",
    "    #  print(f\"The scrambled labels/species are \\n {y_labeled}\")\n",
    "    # print(f\"The corresponding data rows are \\n {X_labeled}\")\n",
    "    \n",
    "    # from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # cross-validation splits the training set into two pieces:\n",
    "    #   + model-building and model-validation. We'll use \"build\" and \"validate\"\n",
    "    #\n",
    "    best_k = 84 # Not correct!\n",
    "    best_accuracy = 0.0  # also not correct...\n",
    "\n",
    "    k_list = []\n",
    "    accuracy_list = []\n",
    "    # Note that we are cross-validating using only our TEST data!\n",
    "    for k in range(1,85):\n",
    "        knn_cv_model = KNeighborsClassifier(n_neighbors=k)   # build knn_model for every k!\n",
    "        cv_scores = cross_val_score( knn_cv_model, X_train, y_train, cv=5 )  # cv=5 means 80/20\n",
    "        # print(cv_scores)  # just to see the five scores... \n",
    "        average_cv_accuracy = cv_scores.mean()  # mean() is numpy's built-in average function \n",
    "        # print(f\"k: {k:2d}  cv accuracy: {average_cv_accuracy:7.4f}\")\n",
    "        \n",
    "    # assign best value of k to best_k\n",
    "        if average_cv_accuracy > best_accuracy:\n",
    "            best_k = k    # at the moment this is incorrect   TO DO for hw4pr1: fix this...\n",
    "            best_accuracy = average_cv_accuracy\n",
    "    # you'll need to use the loop above to find and remember the real best_k\n",
    "        accuracy_list.append(average_cv_accuracy)\n",
    "        k_list.append(k)\n",
    "        \n",
    "    # print(accuracy_list)\n",
    "    # print(k_list)\n",
    "    \n",
    "    return accuracy_list, k_list, best_k\n",
    "\n",
    "    # print(f\"best_k = {best_k}   yields the highest average cv accuracy.\")  # print the best one\n",
    "\n",
    "    \n",
    "    \n",
    "    # knn_model_final = KNeighborsClassifier(n_neighbors=best_k)   # here, we use the best_k\n",
    "    # knn_model_final.fit(X_all, y_all)                              # here we use ALL the data!\n",
    "    # print(f\"Created + trained a 'final' knn classifier, with a (best) k of {best_k}\") \n",
    "    \n",
    "    # #\n",
    "    # # +++ This cell will \"Model-testing Cell\"\n",
    "    # #\n",
    "    # # Now, let's see how well our model does on our \"held-out data\" (the testing data)\n",
    "    # #\n",
    "\n",
    "    # # We run our test set:\n",
    "\n",
    "    # # the function knn_model.predict is the instantiation of our model\n",
    "    # # it's what runs the k-nearest-neighbors algorithm:\n",
    "    # predicted_labels = knn_model.predict(X_test)   \n",
    "    # actual_labels = y_test\n",
    "\n",
    "    # # Let's print them so we can compare...\n",
    "    # # print(\"Predicted labels:\", predicted_labels)\n",
    "    # # print(\"Actual  labels  :\", actual_labels)\n",
    "\n",
    "    # # And, some overall results\n",
    "    # num_correct = sum(predicted_labels == actual_labels)\n",
    "    # total = len(actual_labels)\n",
    "    # print(f\"\\nResults on test set:  {num_correct} correct out of {total} total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9872691276345137, 0.9780768363280956, 0.982317119013608, 0.9773701225471768, 0.9816104052326893, 0.9766583966117836, 0.9766508783800717, 0.9745282309600782, 0.9780693180963838, 0.9731198155527153, 0.9724156078490338, 0.9702954665062776, 0.9717114001453524, 0.9688820389444401, 0.9681778312407587, 0.9653459639626092, 0.9646417562589278, 0.9639300303235345, 0.9639300303235345, 0.9618098889807782, 0.9632208104653784, 0.9603939553417036, 0.9589805277798662, 0.959687241560785, 0.9582763200761848, 0.9582763200761848, 0.9582713079217102, 0.9575620880635543, 0.9547302207854047, 0.952607573365411, 0.952607573365411, 0.952607573365411, 0.9533142871463298, 0.9540235070044858, 0.9511916397263362, 0.950479913790943, 0.9497807182417362, 0.9483647846026614, 0.9504874320226548, 0.9483622785254241, 0.9476580708217426, 0.9469513570408239, 0.9483672906798987, 0.9455379294789866, 0.9455379294789866, 0.9434127759817559, 0.9434102699045186, 0.9448262035435931, 0.9434127759817559, 0.9441194897626746, 0.9412901285617623, 0.9384607673608502, 0.9377540535799314, 0.9356339122371752, 0.936340626018094, 0.936340626018094, 0.933513770894419, 0.9321003433325815, 0.9321003433325815, 0.9271483347116758, 0.9292684760544321, 0.9299802019898253, 0.9292684760544321, 0.9278550484925946, 0.9271508407889131, 0.9257349071498384, 0.9278550484925946, 0.9285642683507506, 0.9292684760544321, 0.9264416209307571, 0.9243189735107636, 0.9236122597298448, 0.9236097536526076, 0.9229005337944516, 0.9221988321680075, 0.9229005337944516, 0.9221913139362956, 0.9221938200135329, 0.9186602511089392, 0.9200736786707766, 0.9200736786707766, 0.9165376036889457, 0.9144174623461895, 0.9122923088489587]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]\n",
      "[0.9801969776708518, 0.9681703130090469, 0.9745382552690274, 0.9710046863644337, 0.9766583966117836, 0.968872014635491, 0.9738240232563967, 0.9695737162619352, 0.9674560809964163, 0.9660401473573416, 0.9639225120918227, 0.9625065784527479, 0.9603864371099917, 0.9603889431872291, 0.9610931508909104, 0.9596797233290729, 0.9582662957672357, 0.9582662957672354, 0.9596822294063104, 0.9589755156253916, 0.9575645941407915, 0.9554394406435607, 0.955441946720798, 0.9568553742826355, 0.9547302207854047, 0.9533117810690925, 0.955434428489086, 0.9519008595844923, 0.954023507004486, 0.9497807182417362, 0.9448287096208304, 0.942706062200837, 0.9398792070771622, 0.9391724932962434, 0.9384657795153247, 0.9377590657344059, 0.9391724932962434, 0.9377615718116432, 0.9391749993734807, 0.9384657795153247, 0.9377590657344059, 0.9363456381725686, 0.9335187830488936, 0.9313986417061374, 0.9321028494098188, 0.9285692805052251, 0.9292759942861439, 0.9299827080670626, 0.9264466330852317, 0.926449139162469, 0.9278600606470692, 0.9243214795880009, 0.9250281933689196, 0.9236122597298448, 0.9229080520261634, 0.92078540460617, 0.9214921183870887, 0.92078540460617, 0.9200786908252512, 0.9172518357015763, 0.9179610555597323, 0.917958549482495, 0.9172518357015763, 0.9172518357015763, 0.9165451219206575, 0.9151342004360575, 0.9144249805779016, 0.9130115530160641, 0.9130165651705386, 0.9115956193769893, 0.9101821918151518, 0.9108889055960706, 0.9108914116733079, 0.9101846978923891, 0.9108863995188333, 0.9101821918151518, 0.9094754780342331, 0.908057038317921, 0.9023933037616219, 0.9016890960579405, 0.9016916021351777, 0.9016916021351777, 0.9023933037616219, 0.9023958098388591]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]\n",
      "[0.9504849259454176, 0.9377490414254568, 0.950479913790943, 0.9441169836854371, 0.9455279051700373, 0.9419918301882065, 0.9405733904718945, 0.9398666766909758, 0.9398641706137383, 0.9356213818509886, 0.9363331077863821, 0.9356263940054633, 0.9342154725208631, 0.9342154725208631, 0.9299802019898253, 0.9264441270079944, 0.9264441270079944, 0.922203844322482, 0.9214971305415632, 0.9207879106834073, 0.922203844322482, 0.9207904167606445, 0.9165426158434202, 0.9179510312507831, 0.915121670049871, 0.9151291882815829, 0.9137132546425081, 0.912294814926196, 0.9108863995188331, 0.9052226649625341, 0.9052251710397714, 0.9052251710397714, 0.9009823822770218, 0.9031075357742525, 0.900984888354259, 0.9009798761997845, 0.899566448637947, 0.8988547227025536, 0.8967370874370347, 0.8960253615016415, 0.8946144400170413, 0.8953286720296718, 0.8939102323133599, 0.8946219582487531, 0.8932035185324411, 0.8910808711124476, 0.8917900909706036, 0.8896699496278474, 0.8903741573315289, 0.8903741573315289, 0.8875447961306167, 0.8861313685687794, 0.8818860737287924, 0.8840112272260232, 0.8818860737287924, 0.8811768538706364, 0.8804701400897177, 0.8811768538706364, 0.8790542064506429, 0.8776407788888054, 0.8783499987469614, 0.875518131468812, 0.8762273513269679, 0.8748114176878932, 0.8734004962032931, 0.8698594090669876, 0.8698594090669876, 0.8684434754279128, 0.8684409693506755, 0.8705661228479062, 0.8684409693506755, 0.8670250357116007, 0.8670250357116007, 0.8656116081497632, 0.8656065959952886, 0.8656091020725258, 0.8627797408716136, 0.8613638072325388, 0.8592436658897826, 0.8585344460316267, 0.8542941633461144, 0.8571210184697892, 0.8564168107661079, 0.8564168107661079]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]\n",
      "[0.8804626218580058, 0.8663133097762072, 0.881866025110894, 0.8818710372653685, 0.8804626218580058, 0.883284464827206, 0.8797458837681379, 0.8797408716136632, 0.878329950129063, 0.8762148209407815, 0.8776232363481442, 0.8726787459589506, 0.8712502819336893, 0.8663082976217327, 0.8677192191063329, 0.8691326466681704, 0.8670150114026514, 0.8677217251835702, 0.8634789364208204, 0.864902388291607, 0.8592336415808337, 0.8585269277999148, 0.856409292534396, 0.8486279227125779, 0.8514547778362529, 0.8465077813698217, 0.8436683958599603, 0.8457960554344284, 0.8458035736661405, 0.8436784201689095, 0.844382627872591, 0.8387264115480038, 0.8365912337418239, 0.8351853244116985, 0.8358870260381425, 0.8344735984763052, 0.8351803122572239, 0.8351803122572239, 0.8330526526827556, 0.8295215898553993, 0.8274014485126431, 0.8281081622935618, 0.8259855148735683, 0.8245720873117308, 0.8203267924717439, 0.8196175726135879, 0.8196200786908252, 0.8217402200335814, 0.8182066511289877, 0.8189083527554318, 0.8203267924717439, 0.8210385184071372, 0.8160890158634689, 0.8139663684434755, 0.809016865899807, 0.8118412149462447, 0.8111345011653259, 0.8097235796807258, 0.8097185675262513, 0.807598426183495, 0.8033556374207453, 0.8097235796807258, 0.8047715710598201, 0.8054832969952134, 0.8026514297170639, 0.8012354960779892, 0.8012354960779892, 0.7998195624389144, 0.8005287822970704, 0.7962809813798462, 0.7962859935343208, 0.7976994210961582, 0.7984086409543143, 0.7948700598952461, 0.7941633461143274, 0.7948750720497206, 0.795579279753402, 0.791333984913415, 0.7927474124752525, 0.7899155451971029, 0.787797909931584, 0.786379470215272, 0.7870911961506654, 0.7821416936069969]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]\n",
      "[0.7637921960754831, 0.7312557953036112, 0.7574142295065535, 0.7609578227200963, 0.7623687442046965, 0.767320752825602, 0.7673132345938901, 0.7623587198957471, 0.7694258577049344, 0.7673007042077037, 0.7680149362203343, 0.7666040147357341, 0.7708518156529583, 0.7652006114828459, 0.7623612259729844, 0.7637746535348221, 0.7581084129012855, 0.7517580131819663, 0.7496228353757863, 0.7475127183419794, 0.7517530010274918, 0.7439791494373857, 0.7432674235019923, 0.736200285692805, 0.7333684184146556, 0.7312482770718993, 0.7333734305691302, 0.7326566924792622, 0.7305315389820313, 0.7305315389820315, 0.7291130992657194, 0.7269929579229631, 0.7276946595494074, 0.7276921534721701, 0.7234518707866577, 0.7234493647094203, 0.722035937147583, 0.7192090820239081, 0.7170914467583891, 0.718499862165752, 0.7177981605393079, 0.7142595794802397, 0.7100117785630153, 0.7149662932611583, 0.7121319199057715, 0.7100067664085408, 0.7071799112848658, 0.7064757035811844, 0.7022304087411974, 0.7036488484575094, 0.7015287071147532, 0.7029421346765907, 0.6979926321329223, 0.69375234944741, 0.6923364158083353, 0.6937548555246471, 0.6852717840763852, 0.6859734857028295, 0.6859734857028295, 0.6866827055609854, 0.6838508382828359, 0.6838458261283613, 0.6838533443600732, 0.6831441245019172, 0.6789038418164048, 0.6781946219582488, 0.6753727789890485, 0.6760719745382552, 0.6753702729118112, 0.6796155677517981, 0.6753677668345739, 0.6732476254918177, 0.6746685712853671, 0.6760819988472045, 0.6739618575044484, 0.6739593514272111, 0.6690098488835426, 0.6725459238653736, 0.6704282885998547, 0.6690173671152545, 0.6683106533343357, 0.6690173671152545, 0.6704282885998547, 0.6683081472570984]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]\n",
      "[0.5961732200586422, 0.5869809287522241, 0.6089116106558403, 0.6110192216124101, 0.6251685336942086, 0.6237626243640829, 0.6258702353206527, 0.6244517956043405, 0.632947397438789, 0.6294063103024834, 0.6357817707941759, 0.6343533067689145, 0.635774252562464, 0.6421497130541562, 0.6371826679698268, 0.6414279628098137, 0.6315164273362905, 0.6322306593489211, 0.6286895722126156, 0.630812219632609, 0.6251635215397338, 0.625163521539734, 0.6202115129188281, 0.6166729318597599, 0.6202140189960654, 0.6124251309425356, 0.6159787484650276, 0.6088990802696539, 0.6096158183595218, 0.6089065985013657, 0.606781445004135, 0.6018369546149412, 0.6004210209758665, 0.5997168132721851, 0.5975941658521916, 0.5983033857103476, 0.5919354434503672, 0.5940631030248352, 0.5912312357466857, 0.5912362479011603, 0.590524521965767, 0.591942961682079, 0.5954740245094354, 0.5940605969475979, 0.591942961682079, 0.5884093927774854, 0.5912387539783975, 0.5926571936947097, 0.5891186126356414, 0.5884118988547227, 0.5855800315765732, 0.5869884469839362, 0.5841540736285493, 0.5834448537703931, 0.5841565797057866, 0.5855675011903866, 0.5912212114377364, 0.589098564017743, 0.5834373355386813, 0.5841515675513118, 0.5792020650076435, 0.5841490614740745, 0.5784903390722501, 0.5834423476931558, 0.5777861313685688, 0.579906272711325, 0.576370197729494, 0.575660977871338, 0.5763676916522568, 0.5756634839485752, 0.5735408365285818, 0.5714206951858256, 0.5721299150439816, 0.5721274089667443, 0.5678846202039947, 0.5692955416885949, 0.5678821141267574, 0.5707114753276696, 0.5700022554695136, 0.5728316166704258, 0.5714181891085883, 0.5721274089667443, 0.5721223968122697, 0.5707089692504324]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]\n",
      "[0.34161842467984865, 0.34941232488785307, 0.3762724607172393, 0.39393779916297017, 0.3932436157682379, 0.3989123624790116, 0.39749392276269957, 0.3783926020599954, 0.38617647795905075, 0.3875999298298374, 0.39679723329073, 0.4045585544946495, 0.4088038493346365, 0.40809212339924317, 0.40950304488384326, 0.4130441320201489, 0.4130441320201489, 0.4151567551311931, 0.4172794025511866, 0.4236373205022178, 0.41586346891211184, 0.4080820990902939, 0.41020224043305015, 0.4172668721650001, 0.4229281006440619, 0.4172668721650001, 0.42363230834774335, 0.42079793499235646, 0.42292058241235, 0.4236222840387941, 0.4200912212114377, 0.4229155702578754, 0.422913064180638, 0.41938200135328174, 0.42008620905696314, 0.41937698919880717, 0.41938200135328174, 0.4186777936496003, 0.42149713054156324, 0.42007618474801395, 0.42290554594892615, 0.42079292283788183, 0.4236147658070822, 0.41866776934065103, 0.41300654086158933, 0.4101821918151517, 0.4172543417788136, 0.4179610555597324, 0.41866526326341374, 0.4087712703305516, 0.41725183570157637, 0.4137182667969827, 0.41301405909330124, 0.42007869082525123, 0.41796606771420697, 0.42079041676064455, 0.4158434202942135, 0.4137232789514573, 0.41796606771420686, 0.4179610555597323, 0.41866776934065103, 0.4158434202942135, 0.4165476279978949, 0.4179610555597323, 0.4158459263714508, 0.41230483923514527, 0.41441996842342677, 0.4165426158434203, 0.41513420043605737, 0.41583840813973894, 0.4186702754178883, 0.4186727814951256, 0.4200837029797258, 0.4165451219206576, 0.4137157607197454, 0.4137182667969827, 0.4130090469388267, 0.42079041676064455, 0.4186702754178883, 0.4151342004360575, 0.41655514622960677, 0.41300654086158933, 0.4116031376087011, 0.4158434202942134]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def line_plot(x):\n",
    "    \"\"\"\n",
    "    creates a line plot of [0-56], [0-48], [0-40], [0-32], [0-24], [0-16], [0-8]\n",
    "    the pixel boxes (scroll to bottom after cell outputs)\n",
    "    \"\"\"\n",
    "    sns.lineplot(x = x[1], y =x[0])\n",
    "    \n",
    "    plt.show\n",
    "\n",
    "digit_7 = line_plot(mask_digit(56))\n",
    "digit_6 = line_plot(mask_digit(48))\n",
    "digit_5 = line_plot(mask_digit(40))\n",
    "digit_4 = line_plot(mask_digit(32))\n",
    "digit_3 = line_plot(mask_digit(24))\n",
    "digit_2 = line_plot(mask_digit(16))\n",
    "digit_1 = line_plot(mask_digit(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  number of pixels    best k\n",
      "------------------  --------\n",
      "                56         1\n",
      "                48         1\n",
      "                40         1\n",
      "                32         6\n",
      "                24        13\n",
      "                16        14\n",
      "                 8        20\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "def best_mask_k():\n",
    "    \"\"\"\n",
    "    creates a ASCII table of number of pixels v best k value\n",
    "    \"\"\"\n",
    "    table = [[56, mask_digit(56)[2]], [48, mask_digit(48)[2]], [40, mask_digit(40)[2]], [32, mask_digit(32)[2]], [24, mask_digit(24)[2]], [16, mask_digit(16)[2]], [8, mask_digit(8)[2]]]\n",
    "    # print(tabulate(table,[\"number of pixels\", \"best k\"], tablefmt=\"grid\"))\n",
    "    print(tabulate(table, headers = [\"number of pixels\", \"best k\"]))\n",
    "\n",
    "best_mask_k()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "- It went great! We noticed that as we decreased the number of pixels, the best k increased. The more pixels, the accuracy seems to decrease linearly. However, when there is less pixels, it peaks at certain points. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
